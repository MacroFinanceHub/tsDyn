\documentclass[a4paper]{article}
%\VignetteIndexEntry{tsDyn: Nonlinear autoregressive time series models in R}
\usepackage[latin9]{inputenc}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{Sweave}

\newcommand{\tsDyn}{\texttt{tsDyn} }
\linespread{1.3}

\begin{document}

\begin{titlepage}
{\centering \huge Nonlinear autoregressive\\[0.5cm]
time series models in R\\[0.5cm]
using \tsDyn version 0.5\\}
\vfill\par
{\centering last revised 21/05/2006 by Antonio, Fabio Di Narzo\\}
\end{titlepage}

\section{Introduction} \tsDyn is an R package for the estimation of
a number of nonlinear time series models.  The package is at an early stage, and
may presumably change significantly in the near future. However, it is
quite usable in the current version.

Each function in the package has at least a minimal help page, with
one or more working examples and detailed explanation of function
arguments and returned values.  In this document we try to
give an overall guided tour of package contents, with some
additional notes which are generally difficult to put in the context
of a manual.

This guide is divided into 3 main sections:
\begin{itemize}
\item Explorative analysis tools
\item Nonlinear autoregressive models
\item A case study
\end{itemize}

\section{Explorative analysis}
\subsection{Bivariate and trivariate relations}
A first explorative analysis should include inspecting the distribution of $(x_t, x_{t-l})$ and that of $(x_t, x_{t-l_1}, x_{t-l_2})$ for some lags $l, l_1, l_2$. This can be done easily in R in a variety of ways. The \tsDyn package provide functions \texttt{autopairs} and \texttt{autotriples} for this purpose.\\
The \texttt{autopairs} function displays, in essence, a scatterplot of time series $x_t$ versus $x_{t-lag}$. The main arguments to the function are the time series and the desired lag. The scatterplot may be also processed to produce bivariate kernel density estimations, as well as nonparametric kernel autoregression estimations. The type of output is governed by the argument \texttt{type}. Possibile values, along with their meanings, are:\\
\begin{tabular}{rl}
\texttt{lines} & directed lines \\
\texttt{points} & simple scatterplot \\
\texttt{levels} & iso-density levels \\
\texttt{persp} & density perspective plot \\
\texttt{image} & density image map \\
\texttt{regression} & kernel autoregression line superposed to scatterplot\\
\end{tabular}
\\For kernel density and regression estimation, you can specify also the kernel window \texttt{h}. 
A typical call to that function can be:
\begin{Schunk}
\begin{Sinput}
 autopairs(x, lag = , type = , h = )
\end{Sinput}
\end{Schunk}
All arguments (except the time series \texttt{x}) have default values.

By default, if running in an interactive environment, the function displays a simple experimental cross-platform GUI, where you can change function parameters and watch interactively how the plot changes.

Similar to \texttt{autopairs}, there is the \texttt{autotriples}
function. This shows $x_t$ versus $(x_{t-lag1}, x_{t-lag2})$, so that
the user has to specify time series $x$ and lags \texttt{lag1} and
\texttt{lag2}. The scatterplot can be processed to produce kernel
regression estimates. Plotting possibilities are:\\
\begin{tabular}{rl} 
\texttt{levels} & iso-values lines\\
\texttt{persp} & perspective plot\\ 
\texttt{image} & image map\\
\texttt{lines} & directed lines \\
\texttt{points} & simple scatterplot \\
\end{tabular}

\subsection{Linearity} An interesting tool for inspecting possible
nonlinearities in the time series is the \emph{locally linear
autoregressive fit} plot, proposed by
Casdagli~\cite{Casdagli1991}. Suppose you think that the dynamical
system underlying your time series is best reconstructed with
embedding dimension $m$ and time delay $d$. Then the locally linear
autoregressive fit plot displays the relative error made by
forecasting time series values with linear models of the form: 
\[x_{t+s} = \phi_0 + \phi_1 x_{t} + \ldots + \phi_m x_{t-(m-1) d} \]
estimated on points in the sphere of radius $\epsilon$ around
$\mathbf{x}^m_t$ for a range of values of $\epsilon$. A minimum
attained at relatively small values of $\epsilon$ may indicate that a
global linear model would be inappropriate for the approximation of
the time series dynamics.  

For this analysis \tsDyn proposes the function \texttt{llar}
which accepts, among others, the following arguments:\\
\begin{tabular}{rl} 
\texttt{x} & time series \\ \
\texttt{m, d, steps} & embedding parameters (see the above model formulation)\\
\end{tabular}\\ 
The function returns a `\texttt{llar}' object, which
can be plotted with the generic \texttt{plot} method. So, a typical
usage would be:\\ 
\begin{Schunk}
\begin{Sinput}
 obj <- llar(log(lynx), m = 3)
 plot(obj)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-003}

However, the \texttt{obj} object can be explicitely converted in an ordinary data.frame:
\begin{Schunk}
\begin{Sinput}
 obj <- data.frame(obj)
\end{Sinput}
\end{Schunk}
with variables:
\begin{Schunk}
\begin{Sinput}
 names(obj)
\end{Sinput}
\begin{Soutput}
[1] "RMSE"    "eps"     "frac"    "avfound"
\end{Soutput}
\end{Schunk}
where `\texttt{RMSE}' stands for Relative Mean Square Error, and \texttt{eps} is enough self-explaining. You can explore this object with usual R commands dedicated to data.frames, such as:
\begin{Schunk}
\begin{Sinput}
 plot(RMSE ~ eps, data = obj, type = "l", log = "x")
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-006}

\subsection{Tests (experimental)}
\tsDyn implements conditional mutual independence and linearity tests as described in Manzan~\cite{Manzan2003}. Function implementations are rather basic, and little tested. Use them carefully!

The \texttt{delta.test} function performs a bootstrap test of independence of $x_t$ versus $x_{t-md}$ conditional on intermediate observations $\{x_{t-d}, \ldots, x_{t-(m-1)d}\}$. The test statistic, available with the function \texttt{delta}, is based on the sample correlation integral, and calls internally the \texttt{d2} function provided by the \texttt{tseriesChaos} package. Among others things, the test requires the specification of a neighborhood window $\epsilon$.\\
Function arguments are the time series \texttt{x}, a vector of
embedding dimensions \texttt{m}, time delay \texttt{d}, a vector of
neighborhood windows \texttt{eps}, the number of bootstrap
replications \texttt{B}. However, default values are available for
\texttt{m, d, eps} and \texttt{B}, so that a typical call can be:
\begin{Schunk}
\begin{Sinput}
 delta.test(x)
\end{Sinput}
\end{Schunk}
The return value is a matrix of p-values, labelled with their associated 
embedding dimensions and neighborhood windows (normally multiple values are tried simultaneously).

The \texttt{delta.lin.test} function performs a bootstrap test of linear dipendence of $x_t$ 
versus $x_{t-md}$ conditional on intermediate observations $\{x_{t-d}, \ldots, x_{t-(m-1)d}\}$. 
The test statistic is available with the function \texttt{delta.lin}. The function arguments 
and returned values are the same as those of \texttt{delta.test}.

\section{Nonlinear autoregressive time series models}
Consider the discrete-time univariate stochastic process $\{X_t\}_{t \in T}$.
Suppose $X_t$ is generated by the map:
\begin{equation}\label{eq:generalNLAR}
X_{t+s} = f(X_t, X_{t-d}, \ldots, X_{t-(m-1)d}; \theta) + \epsilon_{t+s}
\end{equation}
with $\{\epsilon_t\}_{t \in T}$ white noise, $\epsilon_{t+s}$ indipendent w.r.t. $X_{t+s}$, and with $f$ a generic function from $\mathbf{R}^m$ to $\mathbf{R}$.
This class of models is frequently referenced in the literature with the acronym NLAR(m), 
which stands for \emph{NonLinear AutoRegressive} of order $m$.

In \eqref{eq:generalNLAR}, we have implicitely defined the \emph{embedding dimension} $m$, the 
\emph{time delay} $d$ and the \emph{forecasting steps} $s$. The vector $\theta$ indicates a generic vector of parameters governing the shape of $f$, which we would estimate on the basis of some empirical evidence (i.e., an observed time series $\{x_1,x_2,\ldots,x_N\}$).

In \tsDyn some specific NLAR models are implemented. For a list of currently available models, type:
\begin{Schunk}
\begin{Sinput}
 availableModels()
\end{Sinput}
\begin{Soutput}
[1] "linear" "nnetTs" "setar"  "lstar"  "aar"   
\end{Soutput}
\end{Schunk}

Each model can be estimated using a function which takes the name of the model as indicated by 
\texttt{availableModels}. I.e., use \texttt{linear} for fitting a linear model.

All those functions returns an object of base class \texttt{nlar}, from which informations can be extracted using some common methods. Among others:
\begin{verbatim}
print(obj)	#prints basic infos on fitted model and estimated parameters
summary(obj)	#if possible, shows more detailed infos and diagnostics on estimated model
plot(obj)	#shows common diagnostic plots
\end{verbatim}

Another method that can be useful for inspecting the estimated model properties is the \texttt{predict} method:
\begin{Schunk}
\begin{Sinput}
 x.new <- predict(obj, n.ahead = )
\end{Sinput}
\end{Schunk}
This function attempts to extend of \texttt{n.ahead} observations of the original time series used for estimating the model encapsulated in \texttt{obj} using the so called \emph{skeleton} of the fitted model. Assuming that from \eqref{eq:generalNLAR} we estimated $f$ as $\hat{f} = f(\cdot; \hat{\theta})$, using the time series $\mathbf{x} = \{x_1, x_2, \ldots, x_N\}$, we have:
\begin{eqnarray*}
\hat x_{N+1} &=& \hat{f}(x_{N-s}, x_{N - s - d}, \ldots, x_{N - s -(m-1)d})\\
\hat x_{N+2} &=& \hat{f}(x_{N-s+1}, x_{N - s + 1 - d}, \ldots, x_{N - s + 1 -(m-1)d})\\
\ldots \\
\hat x_{N+S} &=& \hat{f}(x_{N-s+(S-1)}, x_{N - s + (S - 1) - d}, \ldots, x_{N - s + (S - 1) -(m-1)d})
\end{eqnarray*}

A detailed description of some actually implemented models follows.

\subsection{Linear models}
\begin{equation}\label{eq:linear}
X_{t+s} = \phi + \phi_0 X_t + \phi_1 X_{t-d} + \ldots + \phi_m X_{t-(m-1)d} + \epsilon_{t+s}
\end{equation}
It's a classical AR(m) model, and its specification doesn't require additional hyper-parameters. 
Estimation is done via CLS (Conditional Least Squares). 
The summary command returns asymptotics standard errors for the estimated coefficients, based on the normality assumption of residuals.

Note that in R there are plenty of functions for AR (and, more generally, ARMA) models estimation, with different estimation methods, such as ML, CLS, Yule-Walker, $\ldots$. If you really need to fit linear models, use these methods directly.

\subsection{SETAR models}
\begin{equation}
X_{t+s} = \left\{
\begin{array}{lr}
\phi_1 + \phi_{10} X_t + \phi_{11} X_{t-d} + \ldots + \phi_{1L} X_{t-(L-1)d} + \epsilon_{t+s} & Z_t \leq c\\
\\
\phi_2 + \phi_{20} X_t + \phi_{21} X_{t-d} + \ldots + \phi_{2H} X_{t-(H-1)d} + \epsilon_{t+s} & Z_t > c
\end{array} \right.
\end{equation}
with $Z_t$ a threshold variable. How is one to define $Z_t$? 
Strictly speaking, in SETAR models $Z_t$ should be one of $\{X_t, X_{t-d}, X_{t-(m-1)d}\}$. 
We can define the threshold variable $Z_t$ via the \emph{threshold delay} $\delta$, 
such that 
\[Z_t = X_{t-\delta d}\] 
Using this formulation, you can specify SETAR models with:
\begin{Schunk}
\begin{Sinput}
 obj <- setar(x, m = , d = , steps = , thDelay = )
\end{Sinput}
\end{Schunk}
where \texttt{thDelay} stands for the above defined $\delta$, and must be an integer number between $0$ and $m-1$.\\
For greater flexibility, you can also define the threshold variable as an arbitrary linear combination of lagged time series values:
\[
Z_t = \beta_1 X_t + \beta_2 X_{t-1} + \ldots + \beta_m X_{t-(m-1)d}
\]
In R this is implemented as follows:
\begin{Schunk}
\begin{Sinput}
 obj <- setar(x, m = , d = , steps = , mTh = )
\end{Sinput}
\end{Schunk}
where \texttt{mTh} stands for $\beta$, and takes the form of a vector of real coefficients of length $m$.\\
Finally, $Z_t$ can be an external variable. This is obtained with the call:
\begin{Schunk}
\begin{Sinput}
 obj <- setar(x, m = , d = , steps = , thVar = )
\end{Sinput}
\end{Schunk}
where \texttt{thVar} is the vector containing the threshold variable values.

The threshold variable isn't the only additional parameter governing the SETAR model. One can specify the \emph{low} and \emph{high} regime autoregressive orders $L$ and $H$. These can be specified with the arguments \texttt{mL} and \texttt{mH}, respectively:
\begin{Schunk}
\begin{Sinput}
 obj <- setar(x, m = , d = , steps = , thDelay = , mL = , mH = )
\end{Sinput}
\end{Schunk}
If not specified, \texttt{mL} and \texttt{mH} defaults to \texttt{m}.

Another hyper-parameter one can specify is the threshold value $c$, via the additional argument \texttt{th}.
If not specified, this is estimated by fitting the model for a grid of different, reasonable values of $c$, and taking the best fit as the final $c$ estimate.

Note that, conditional on $\{Z_t\leq c\}$, the model is linear. So, for a fixed threshold value, the CLS estimation is straightforward.

The summary command for this model returns asymptotic standard errors for the estimated $\phi$ coefficients, based on the assumption that $\epsilon_t$ are normally distributed.

\subsection{LSTAR models}
The LSTAR model can be viewed as a generalization of the above defined SETAR model:
\begin{eqnarray*}
X_{t+s} =& 
( \phi_1 + \phi_{10} X_t + \phi_{11} X_{t-d} + \ldots + \phi_{1L} X_{t-(L-1)d} )
	( 1 - G(Z_t, \gamma, c) ) \\
	 & + ( \phi_2 + \phi_{20} X_t + \phi_{21} X_{t-d} + \ldots + \phi_{2H} X_{t-(H-1)d} )
	 G( Z_t, \gamma, c) + \epsilon_{t+s}
\end{eqnarray*}
with $G$ the logistic function, and $Z_t$ the threshold variable. For $Z_t$, $L$ and $H$ specification, the same convention as that of SETAR models is followed. In addition, for LSTAR models one has to specify some starting values for all the parameters to be estimated: $(\phi, \gamma, c)$.
If not provided, $\phi$ and $c$ starting values are taken from the corresponding SETAR model estimation.

Once again, estimation is done using CLS, but here an exact solution does not extist, so a numerical minimization algorithm has to be used.

Standard errors provided in the summary are asymptotical, but may not make sense if the error criterion minimization isn't achieved during parameter estimation. Be aware of this!

\subsection{Neural Network models}
A neural network model with linear output, $D$ \emph{hidden units} and activation function $g$, is represented as:
\begin{equation}
x_{t+s} = \beta_0 + \sum_{j=1}^D \beta_j g( \gamma_{0j} + \sum_{i=1}^{m} \gamma_{ij} x_{t-(i-1) d} )
\end{equation}

For the implementation the \texttt{nnet} package is used, so please refer to the \texttt{nnet} package documentation for more details.

The only additional argument for specifying this model is the number of hidden units \texttt{size}, which stands for the above defined $D$:
\begin{Schunk}
\begin{Sinput}
 obj <- nnetTs(x, m = , d = , steps = , size = )
\end{Sinput}
\end{Schunk}

The estimation is done via CLS. No additional summary informations are available for this model.

\subsection{Additive Autoregressive models}
A non-parametric additive model (a GAM, Generalized Additive Model), of the form:
\begin{equation}
x_{t+s} = \mu + \sum_{i=1}^{m} s_i ( x_{t-(i-1) d} )
\end{equation}
where $s_i$ are smooth functions represented by penalized cubic regression splines. They are estimated, along with their degree of smoothing, using the \texttt{mgcv} package~\cite{Wood2004}.

No additional parameters are required for this model:
\begin{Schunk}
\begin{Sinput}
 obj <- aar(x, m = , d = , steps = )
\end{Sinput}
\end{Schunk}

Some diagnostic plots and summaries are provided for this model, adapted from those produced by \texttt{mgcv}.

\subsection{Model selection}
A common task in time series modelling is \emph{fine tuning} of the hyper-parameters. R is a complete programming language, so the user can easily define his error criterion, fit a set of models and chose the best between them. However, the \tsDyn package provides some helpful functions for this task.

For SETAR models, there is the \texttt{selectSETAR} function. The time series, the embedding parameters and a vector of values for each provided hyper-parameter is passed to this function. The routine then tries to fit the model for the full grid of hyper-parameter values, and gives as output a list of the best combinations found. So, for example:
\begin{Schunk}
\begin{Sinput}
 x <- log10(lynx)
 selectSETAR(x, m = 3, mL = 1:3, mH = 1:3, thSteps = 5, thDelay = 0:2)
\end{Sinput}
\begin{Soutput}
   thDelay       th mL mH pooled.AIC
1        2 2.886993  3  2  -40.83722
2        2 2.886993  3  3  -39.72782
3        2 2.886993  2  2  -34.36648
4        1 3.276573  3  2  -34.04683
5        2 2.886993  2  3  -33.25708
6        2 2.886993  1  2  -32.42552
7        1 3.276573  3  3  -32.16499
8        2 2.886993  1  3  -31.31612
9        1 3.276573  2  2  -30.36645
10       1 2.886993  3  2  -28.84706
\end{Soutput}
\end{Schunk}
tries to fit $3 \times 3 \times 3 \times 5$ models, one for each combination of \texttt{mL},\texttt{mH},\texttt{thDelay} and \texttt{th}, and returns the best combinations w.r.t. the AIC criterion.

Totally analogous are the \texttt{selectLSTAR} and \texttt{selectNNET} functions, for which we refer to the online documentation.

\section{Case study}
We herein analyse the Canadian lynx data set.
This consists of annual records of the numbers of the Canadian lynx trapped in the Mackenzie River district of North-west Canada for the period 1821-1934.

The time series, named \texttt{lynx}, is available in a default R installation, so one can type directly, in an R session:
\begin{Schunk}
\begin{Sinput}
 str(lynx)
\end{Sinput}
\begin{Soutput}
 Time-Series [1:114] from 1821 to 1934:  269  321  585  871 1475 ...
\end{Soutput}
\begin{Sinput}
 summary(lynx)
\end{Sinput}
\begin{Soutput}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   39.0   348.2   771.0  1538.0  2567.0  6991.0 
\end{Soutput}
\begin{Sinput}
 plot(lynx)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-017}

Here we will roughly follow the analysis in Tong~\cite{Tong1990}.

\subsection{Explorative analysis}
First, we log transform the data:
\begin{Schunk}
\begin{Sinput}
 x <- log10(lynx)
\end{Sinput}
\end{Schunk}

Plot of the time series and time-inverted time series:
\begin{Schunk}
\begin{Sinput}
 par(mfrow = c(2, 1), mar = c(0, 0, 0, 0))
 plot(x, ax = F)
 box()
 plot(x[length(x):1], type = "l", ax = F)
 box()
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-019}

Nonparametric regression function of $X_t$ versus $X_{t-1}$ and of $X_t$ versus $X_{t-3}$ (kernel estimation):
\begin{Schunk}
\begin{Sinput}
 par(mfrow = c(2, 1), mar = c(2, 2, 0, 0))
 autopairs(x, lag = 1, type = "regression", GUI = FALSE)
 autopairs(x, lag = 3, type = "regression", GUI = FALSE)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-020}

For lag 3 (bottom plot), a linear approximation for the regression function may be questionable.

The marginal histogram of data shows bimodality:
\begin{Schunk}
\begin{Sinput}
 hist(x, br = 13)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-021}

Global and partial autocorrelation:
\begin{Schunk}
\begin{Sinput}
 par(mfrow = c(2, 1), mar = c(2, 4, 0, 0))
 acf(x)
 pacf(x)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-022}

The \texttt{tseriesChaos} package offers some other explorative tools tipycal of nonlinear time series analysis. The Average Mutual Information (see online help for further explanation):
\begin{Schunk}
\begin{Sinput}
 library(tseriesChaos)
 mutual(x)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-023}

Recurrence plot (see online help):
\begin{Schunk}
\begin{Sinput}
 recurr(x, m = 3, d = 1, levels = c(0, 0.2, 1))
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-024}

From this plot, deterministic cycles appears from the embedding-reconstructed underlying dynamics.

Directed lines are a tipycal tool for time series explorations. The \texttt{lag.plot} function in the base \texttt{stats} package does this well:
\begin{Schunk}
\begin{Sinput}
 lag.plot(x, lags = 3, layout = c(1, 3))
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-025}

Especially for lag 2, a cycle is again evident. Moreover, the void space in the middle is a typical argument for rejecting the bivariate normality of $(X_t, X_{t-l})$.

What follows is the application of still-experimental code for testing the conditional mutual independence and linearity for lags 2 and 3:
\begin{Schunk}
\begin{Sinput}
 delta.test(x)
\end{Sinput}
\begin{Soutput}
   eps
m   0.2792 0.5584 0.8376 1.1168
  2   0.02   0.02   0.08   0.02
  3   0.20   0.02   0.02   0.02
\end{Soutput}
\begin{Sinput}
 delta.lin.test(x)
\end{Sinput}
\begin{Soutput}
   eps
m   0.2792 0.5584 0.8376 1.1168
  2   0.26   0.20   0.64   0.52
  3   0.24   0.28   0.20   0.50
\end{Soutput}
\end{Schunk}
P-values are reported, labelled with their embedding dimensions $m$
and \emph{window} values $\epsilon$.
We reject conditional independence quite easily. There is some trouble instead for deciding to reject or not linearity. See Manzan~\cite{Manzan2003} for a detailed discussion on these tests.

\subsection{Model selection}
The first model proposed in the literature for these data was an AR(2): 
\[
X_t = 1.05 + 1.41 X_{t-1} - 0.77 X_{t-2} + \epsilon_t
\]
with $v(\epsilon_t)=\sigma^2=0.04591$.

This can be estimated with \tsDyn using the command:
\begin{Schunk}
\begin{Sinput}
 mod.ar <- linear(x, m = 2)
 mod.ar
\end{Sinput}
\begin{Soutput}
Non linear autoregressive model

AR model
Coefficients:
(intercept)       phi.1       phi.2 
  1.0576005   1.3842377  -0.7477757 
\end{Soutput}
\end{Schunk}

As an improvement to the AR model, we may consider applying a SETAR(2; 2,2) model with threshold delay $\delta=1$. In R:
\begin{Schunk}
\begin{Sinput}
 mod.setar <- setar(x, m = 2, mL = 2, mH = 2, thDelay = 1)
 mod.setar
\end{Sinput}
\begin{Soutput}
Non linear autoregressive model

SETAR model (2 regimes)
Coefficients:
Low regime:
    phi1.0     phi1.1     phi1.2 
 0.5884369  1.2642793 -0.4284292 

High regime:
   phi2.0    phi2.1    phi2.2 
 1.165692  1.599254 -1.011575 

Threshold
Variable: Z(t) = + (0) X(t) + (1) X(t-1)
Value: 3.318
Proportion of points in low regime: 69.6%
\end{Soutput}
\end{Schunk}

So, the fitted model may be written as:
\[
X_{t+1} = \left\{\begin{array}{lr}
0.588 + 1.264 X_{t} -0.428 X_{t-1} & X_{t-1} \leq 3.318 \\
1.166 + 1.599 X_{t} -1.012 X_{t-1} & X_{t-1} > 3.318 \\
\end{array}\right.
\]

For an automatic comparison, we may fit different linear and nonlinear models and directly compare some measures of their fit:
\begin{Schunk}
\begin{Sinput}
 mod <- list()
 mod[["linear"]] <- linear(x, m = 2)
 mod[["setar"]] <- setar(x, m = 2, thDelay = 1)
 mod[["lstar"]] <- lstar(x, m = 2, thDelay = 1)
\end{Sinput}
\begin{Soutput}
Using maximum autoregressive order for low regime: mL = 2 
Using maximum autoregressive order for high regime: mH = 2 
Missing starting values. Using SETAR estimations:
    phi1.0     phi1.1     phi1.2     phi2.0     phi2.1     phi2.2         th 
 0.5884369  1.2642793 -0.4284292  1.1656919  1.5992541 -1.0115755  3.3179135 
Missing starting value for 'gamma'. Using gamma =  8 
Convergence problem. Convergence code:  1 
\end{Soutput}
\begin{Sinput}
 mod[["nnetTs"]] <- nnetTs(x, m = 2, size = 3)
\end{Sinput}
\begin{Soutput}
# weights:  13
initial  value 1123.766049 
iter  10 value 7.776995
iter  20 value 5.136185
iter  30 value 4.711834
iter  40 value 4.563658
iter  50 value 4.438878
iter  60 value 4.422297
iter  70 value 4.421858
iter  80 value 4.421757
iter  90 value 4.421338
iter 100 value 4.421303
final  value 4.421303 
stopped after 100 iterations
\end{Soutput}
\begin{Sinput}
 mod[["aar"]] <- aar(x, m = 2)
\end{Sinput}
\end{Schunk}

Now the \texttt{mod} object contains a labelled list of fitted \texttt{nlar} models.
As an example, we can compare them in term of the AIC and MAPE index:
\begin{Schunk}
\begin{Sinput}
 sapply(mod, AIC)
\end{Sinput}
\begin{Soutput}
   linear     setar     lstar    nnetTs       aar 
-121.8737 -358.3740 -355.7803 -344.4731 -328.0814 
\end{Soutput}
\begin{Sinput}
 sapply(mod, MAPE)
\end{Sinput}
\begin{Soutput}
    linear      setar      lstar     nnetTs        aar 
0.06801955 0.05648596 0.05567342 0.05709281 0.05951105 
\end{Soutput}
\end{Schunk}

From this comparison, the SETAR model seems to be the best.

More detailed diagnostics can be extracted:
\begin{Schunk}
\begin{Sinput}
 summary(mod[["setar"]])
\end{Sinput}
\begin{Soutput}
Non linear autoregressive model

SETAR model (2 regimes)
Coefficients:
Low regime:
    phi1.0     phi1.1     phi1.2 
 0.5884369  1.2642793 -0.4284292 

High regime:
   phi2.0    phi2.1    phi2.2 
 1.165692  1.599254 -1.011575 

Threshold
Variable: Z(t) = + (0) X(t) + (1) X(t-1)
Value: 3.318
Proportion of points in low regime: 69.6%

Residuals:
      Min        1Q    Median        3Q       Max 
-0.571121 -0.109431  0.017641  0.116468  0.516270 

Fit:
residuals variance = 0.03814,  AIC = -358, MAPE = 5.649%

Coefficient(s):

        Estimate  Std. Error  t value  Pr(>|t|)    
phi1.0  0.588437    0.143307   4.1061 7.844e-05 ***
phi1.1  1.264279    0.065256  19.3741 < 2.2e-16 ***
phi1.2 -0.428429    0.077487  -5.5291 2.260e-07 ***
phi2.0  1.165692    0.876606   1.3298 0.1863928    
phi2.1  1.599254    0.108966  14.6767 < 2.2e-16 ***
phi2.2 -1.011575    0.265011  -3.8171 0.0002255 ***
---
Signif. codes:  0  

Threshold
Variable: Z(t) = + (0) X(t) + (1) X(t-1)

Value: 3.318
Proportion of points in low regime: 69.6%
\end{Soutput}
\end{Schunk}

More diagnostic plots can be displayed using the command:
\begin{Schunk}
\begin{Sinput}
 plot(mod[["setar"]])
\end{Sinput}
\end{Schunk}

\subsection{Out-of-sample forecasting}
Fit models on first 104 observations:

\begin{Schunk}
\begin{Sinput}
 set.seed(10)
 mod.test <- list()
 x.train <- window(x, end = 1924)
 x.test <- window(x, start = 1925)
 mod.test[["linear"]] <- linear(x.train, m = 2)
 mod.test[["setar"]] <- setar(x.train, m = 2, thDelay = 1)
 mod.test[["lstar"]] <- lstar(x.train, m = 2, thDelay = 1, trace = FALSE, 
+     control = list(maxit = 1e+05))
 mod.test[["nnet"]] <- nnetTs(x.train, m = 2, size = 3, control = list(maxit = 1e+05))
\end{Sinput}
\begin{Soutput}
# weights:  13
initial  value 1009.386247 
iter  10 value 5.610020
iter  20 value 5.422071
iter  30 value 5.408916
iter  40 value 5.319344
iter  50 value 5.080814
iter  60 value 4.862847
iter  70 value 4.596344
iter  80 value 4.436638
iter  90 value 4.296703
iter 100 value 4.266607
final  value 4.266607 
stopped after 100 iterations
\end{Soutput}
\begin{Sinput}
 mod.test[["aar"]] <- aar(x.train, m = 2)
\end{Sinput}
\end{Schunk}

Compare forecasts with real last 10 observed values:
\begin{Schunk}
\begin{Sinput}
 frc.test <- lapply(mod.test, predict, n.ahead = 10)
 plot(x.test, ylim = range(x))
 for (i in 1:length(frc.test)) lines(frc.test[[i]], lty = i + 
+     1, col = i + 1)
 legend(1925, 2.4, lty = 1:(length(frc.test) + 1), col = 1:(length(frc.test) + 
+     1), legend = c("observed", names(frc.test)))
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-036}

From this visual comparison, the SETAR(2; 2,2) model seems to be one of the bests.

\subsection{Inspecting model skeleton}
An interesting task can be inspecting the fitted model skeleton.

This can be achieved by comparing the forecasting results under each model.
\begin{Schunk}
\begin{Sinput}
 x.new <- predict(mod[["linear"]], n.ahead = 100)
 lag.plot(x.new, 1)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-038}

A fixed point, i.e. the only possible stationary solution with a linear model.
\begin{Schunk}
\begin{Sinput}
 x.new <- predict(mod[["setar"]], n.ahead = 100)
 lag.plot(x.new, 1)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-039}

A stable periodic cycle.

\begin{Schunk}
\begin{Sinput}
 x.new <- predict(mod[["nnet"]], n.ahead = 100)
 lag.plot(x.new, 1)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-040}

Appears to be a quasiperiodic cycle lying on an invariant curve.

\section{Sensitivity on initial conditions}
In the previous section we observed skeletons with cyclical or limit fixed point behaviour.

Neural networks and SETAR models can explain also different types of attractors.
For this data set, Tong~\cite{Tong1990} showed that particular types of SETAR models 
can yeld to fixed limit points as well as unstable orbits and \emph{possibly chaotic} systems.

For example, a fixed limit point:
\begin{Schunk}
\begin{Sinput}
 mod.point <- setar(x, m = 10, mL = 3, mH = 10, thDelay = 0, th = 3.12)
 lag.plot(predict(mod.point, n.ahead = 100))
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-041}

Unstable orbit:
\begin{Schunk}
\begin{Sinput}
 mod.unstable <- setar(x, m = 9, mL = 9, mH = 6, thDelay = 4, 
+     th = 2.61)
 lag.plot(predict(mod.unstable, n.ahead = 100))
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-042}

Possibly chaotic systems:
\begin{Schunk}
\begin{Sinput}
 mod.chaos1 <- setar(x, m = 5, mL = 5, mH = 3, thDelay = 1, th = 2.78)
 lag.plot(predict(mod.chaos1, n.ahead = 100))
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-043}
\begin{Schunk}
\begin{Sinput}
 mod.chaos2 <- setar(x, m = 5, mL = 5, mH = 3, thDelay = 1, th = 2.95)
 lag.plot(predict(mod.chaos2, n.ahead = 100))
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-044}

For a given fitted model, we can try estimating the maximal Lyapunov exponent with the Kantz algorithm 
using the \texttt{lyap\_k} function in the \texttt{tseriesChaos} package~\cite{tseriesChaos2005,TISEAN1999}. 
This function takes as input an observed time series, so we can procede as follows:
\begin{enumerate}
\item generate N observations from the model
\item add a little observational noise (otherwise the Kantz algorithm will fail)
\item apply the \texttt{lyap\_k} function to the generated time series
\end{enumerate}

Follows the R code for analysing the selected SETAR(2; 2,2) model of the previous paragraph and the possibly chaotic 
SETAR(2; 5,3) just seen above.
\begin{Schunk}
\begin{Sinput}
 N <- 1000
 x.new <- predict(mod[["setar"]], n.ahead = N)
 x.new <- x.new + rnorm(N, sd = sd(x.new)/100)
 ly <- lyap_k(x.new, m = 2, d = 1, t = 1, k = 2, ref = 750, s = 200, 
+     eps = sd(x.new)/10)
\end{Sinput}
\begin{Soutput}
Finding nearests
Keeping  741  reference points
Following points
\end{Soutput}
\begin{Sinput}
 plot(ly)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-045}

There is no scaling region, so the maximal Lyapunov exponent can assumed to be $\leq 0$.
\begin{Schunk}
\begin{Sinput}
 x.new <- predict(mod.chaos2, n.ahead = N)
 x.new <- x.new + rnorm(N, sd = sd(x.new)/100)
 ly <- lyap_k(x.new, m = 5, d = 1, t = 1, k = 2, ref = 750, s = 200, 
+     eps = sd(x.new)/10)
\end{Sinput}
\begin{Soutput}
Finding nearests
Keeping  732  reference points
Following points
\end{Soutput}
\begin{Sinput}
 plot(ly)
\end{Sinput}
\end{Schunk}
\includegraphics{tsDyn-046}

Here there is a scaling region. The final $\lambda$ estimate for this time series is the slope of the plotted curve in that region:
\begin{Schunk}
\begin{Sinput}
 lyap(ly, start = 6, end = 70)
\end{Sinput}
\begin{Soutput}
(Intercept)      lambda 
-4.53873645  0.01979556 
\end{Soutput}
\end{Schunk}

At this point a natural question can be: 
\emph{why not use directly the original time series as input to \texttt{lyap\_k} instead of model-generated observations}?
The answer here is that we have a too short time series for succesfully applying the Kantz algorithm, 
so a preliminary modelling for generating more observations is necessary.
\nocite{*}
\bibliographystyle{amsplain}

\bibliography{bib}
\end{document}
